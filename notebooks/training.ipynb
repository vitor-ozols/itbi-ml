{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7f35e46",
   "metadata": {},
   "source": [
    "\n",
    "### Modelo Hedônico IPTU — Treinamento Multifaixas com CatBoost\n",
    "\n",
    "Este notebook realiza o treinamento de um **modelo hedônico** para estimar o valor de transação (ITBI/IPTU),\n",
    "utilizando **CatBoostRegressor** com variáveis cadastrais do imóvel e **agregados mensais defasados (lag-1)**\n",
    "por **setor fiscal** e **grupo de uso** (TERRENO vs EDIFICADO).\n",
    "\n",
    "A abordagem é **multifaixas**: treina-se um modelo por combinação de:\n",
    "- `grupo_uso` ∈ {`TERRENO`, `EDIFICADO`}\n",
    "- `faixa` ∈ faixas de valor (<=500k, 500k–2mi, 2mi–10mi, >10mi)\n",
    "\n",
    "A variável-alvo é modelada em **log** (`log1p(valor / escala)`) para estabilizar variância e melhorar ajuste.\n",
    "\n",
    "%% [markdown]\n",
    "### 1) Imports\n",
    "\n",
    "- **glob / Path**: varredura recursiva dos Parquets na camada staging.\n",
    "- **pandas / numpy**: manipulação de dados e engenharia de atributos.\n",
    "- **catboost**: regressão com suporte nativo a variáveis categóricas.\n",
    "- **sklearn**: validação cruzada (KFold) e métricas (MAE e R²)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb73bcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U scikit-learn catboost -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "def6b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b036497",
   "metadata": {},
   "source": [
    "### 2) Configurações\n",
    "\n",
    "- `STAGING_PATH`: caminho para os Parquets já padronizados (camada staging).\n",
    "- `ARTEFATOS_PATH`: diretório para salvar modelos e metadados do treinamento.\n",
    "- `TARGET_SCALE`: escala aplicada antes do log para melhorar estabilidade numérica.\n",
    "\n",
    "#### Multifaixas\n",
    "As faixas são definidas por `FAIXAS_BINS` e `FAIXAS_LABELS`, utilizadas para:\n",
    "- segmentar o treinamento por nível de preço\n",
    "- permitir modelos especializados em regimes distintos (ex.: imóveis baratos vs caros)\n",
    "\n",
    "#### Variáveis categóricas\n",
    "`CAT_COLS` são passadas explicitamente ao CatBoost via `cat_features` (sem necessidade de one-hot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7e67ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRONZE_PATH = \"../data/bronze/itbi\"\n",
    "ARTEFATOS_PATH = \"../data/artefatos_modelo_multifaixas\"\n",
    "Path(ARTEFATOS_PATH).mkdir(exist_ok=True)\n",
    "\n",
    "TARGET_SCALE = 1000.0\n",
    "\n",
    "FAIXAS_BINS = [0, 500_000, 2_000_000, 10_000_000, 30_000_000, 100_000_000, np.inf]\n",
    "FAIXAS_LABELS = [\n",
    "  \"faixa1_<=500k\",\n",
    "  \"faixa2_500k_2mi\",\n",
    "  \"faixa3_2mi_10mi\",\n",
    "  \"faixa4_10mi_30mi\",\n",
    "  \"faixa5_30mi_100mi\",\n",
    "  \"faixa6_>100mi\"\n",
    "]\n",
    "\n",
    "\n",
    "CAT_COLS = [\n",
    "    \"setor_fiscal\",\n",
    "    \"descricao_uso_iptu\",\n",
    "    \"descricao_padrao_iptu\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77234859",
   "metadata": {},
   "source": [
    "### 3) Carregar Parquets (camada staging)\n",
    "\n",
    "- Varre recursivamente todos os arquivos `.parquet` em `STAGING_PATH`.\n",
    "- Concatena em um único DataFrame para treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de65dbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Carregando parquets...\n",
      "Base carregada: 2600551 linhas\n"
     ]
    }
   ],
   "source": [
    "print(\">> Carregando parquets...\")\n",
    "files = glob.glob(f\"{BRONZE_PATH}/**/*.parquet\", recursive=True)\n",
    "df = pd.concat([pd.read_parquet(f) for f in files], ignore_index=True)\n",
    "print(\"Base carregada:\", len(df), \"linhas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6ce919",
   "metadata": {},
   "source": [
    "### 4) Filtros de qualidade e recorte do domínio\n",
    "\n",
    "#### Objetivo\n",
    "Aplicar regras de elegibilidade e consistência para reduzir ruído e manter apenas registros comparáveis ao fenômeno modelado (transações válidas e atributos coerentes).\n",
    "\n",
    "#### Estratégia\n",
    "Os filtros são aplicados em **etapas**, e a cada etapa registramos:\n",
    "- linhas **antes / depois**\n",
    "- linhas **perdidas**\n",
    "- **critério** aplicado\n",
    "\n",
    "Esse log é retornado como um dicionário `audit` (auditoria) e pode ser salvo em CSV/JSON.\n",
    "\n",
    "#### Regras aplicadas (por etapa)\n",
    "\n",
    "- **TN (natureza da transação)**  \n",
    "  Mantém **compra e venda**; opcionalmente inclui **permuta** (`include_permuta=True`).\n",
    "\n",
    "- **STATUS (situação do SQL)**  \n",
    "  Mantém `situacao_sql ∈ {Ativo Territorial}`.\n",
    "\n",
    "- **VALOR (faixa de valor declarado)**  \n",
    "  Mantém `min_valor ≤ valor_transacao_declarado ≤ max_valor`.\n",
    "\n",
    "- **PROP (proporção transmitida)**  \n",
    "  Mantém `0 < proporcao_transmitida_percent ≤ 100`.\n",
    "\n",
    "- **AREAS (consistência área vs tipo do imóvel)** *(heurística via `descricao_uso_iptu`)*  \n",
    "  - Terreno → `area_terreno_m2 > 0`  \n",
    "  - Edificado → `area_construida_m2 > 0`  \n",
    "  - Opcional: se `allow_land_on_edificado=False`, remove edificados com `area_terreno_m2 > 0` (não recomendado na maioria dos casos).\n",
    "\n",
    "- **ANO_CONS (ano de conclusão da construção)**  \n",
    "  Mantém `ano_conclusao_construcao_iptu ∈ [ano_min, ano_max]` ou NA.\n",
    "\n",
    "- **DATA (data da transação plausível)**  \n",
    "  Mantém `data_transacao ∈ [2000..hoje]` ou NA.\n",
    "\n",
    "- **DEDUP (deduplicação)**  \n",
    "  Remove duplicatas por `['cadastro_sql', 'data_transacao', 'valor_transacao_declarado']` (quando presentes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0e1bec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] inicial | antes=2,600,551 → depois=2,600,551 | perdidos=0\n",
      "[TN] natureza_transacao == \"1.Compra e venda\" | antes=2,600,551 → depois=2,294,635 | perdidos=305,916\n",
      "[STATUS] situacao_sql ∈ {\"Ativo Predial\",\"Ativo Territorial\"} | antes=2,294,635 → depois=2,248,884 | perdidos=45,751\n",
      "[VALOR] 100,000 ≤ valor_transacao_declarado ≤ 100,000,000 | antes=2,248,884 → depois=1,745,860 | perdidos=503,024\n",
      "[PROP] 0 < proporcao_transmitida_percent ≤ 100 (ou NA) | antes=1,745,860 → depois=1,745,490 | perdidos=370\n",
      "[AREAS] terreno → land>0; edificado → built>0 | antes=1,745,490 → depois=1,745,490 | perdidos=0\n",
      "[ANO_CONS] ano_conclusao_construcao_iptu ∈ [1900, 2026] ou NA | antes=1,745,490 → depois=1,745,484 | perdidos=6\n",
      "[DATA] data_transacao ∈ [2000..hoje] ou NA | antes=1,745,484 → depois=1,745,446 | perdidos=38\n",
      "[DEDUP] drop_duplicates por ['cadastro_sql', 'data_transacao', 'valor_transacao_declarado'] | antes=1,745,446 → depois=1,684,944 | perdidos=60,502\n",
      "[FIM] linhas iniciais=2,600,551 | finais=1,684,944 | perda=35.21%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def filtrar_base(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    include_permuta: bool = False,\n",
    "    min_valor: float = 5_000.0,\n",
    "    max_valor: float = 50_000_000.0,\n",
    "    ano_min: int = 1900,\n",
    "    ano_max: int | None = None,\n",
    "    situacao = [\"Ativo Predial\", \"Ativo Territorial\"],\n",
    "    allow_land_on_edificado: bool = True,\n",
    "    verbose: bool = True,\n",
    ") -> tuple[pd.DataFrame, dict]:\n",
    "\n",
    "\n",
    "    if ano_max is None:\n",
    "        ano_max = datetime.today().year\n",
    "\n",
    "    def _count(stage, df_before, df_after, why, stats):\n",
    "        b, a = len(df_before), len(df_after)\n",
    "        drop = b - a\n",
    "        stats.append({\"etapa\": stage, \"antes\": b, \"depois\": a, \"perdidos\": drop, \"criterio\": why})\n",
    "        if verbose:\n",
    "            print(f\"[{stage}] {why} | antes={b:,} → depois={a:,} | perdidos={drop:,}\")\n",
    "\n",
    "    stats: list[dict] = []\n",
    "    df0 = df.copy()\n",
    "    _count(\"START\", df0, df0, \"inicial\", stats)\n",
    "\n",
    "    # -------- 1) natureza_transacao\n",
    "    if \"natureza_transacao\" in df0.columns:\n",
    "        df_before = df0\n",
    "        tn = df0[\"natureza_transacao\"].astype(str).str.strip()\n",
    "\n",
    "        if include_permuta:\n",
    "            mask = tn.eq(\"1.Compra e venda\") | tn.str.contains(\"permuta\", case=False, na=False)\n",
    "            why = 'natureza_transacao ∈ {\"1.Compra e venda\", \"*permuta*\"}'\n",
    "        else:\n",
    "            mask = tn.eq(\"1.Compra e venda\")\n",
    "            why = 'natureza_transacao == \"1.Compra e venda\"'\n",
    "\n",
    "        df0 = df0[mask]\n",
    "        _count(\"TN\", df_before, df0, why, stats)\n",
    "\n",
    "    # -------- 2) situacao_sql (DOIS casos)\n",
    "    if \"situacao_sql\" in df0.columns:\n",
    "        df_before = df0\n",
    "        st = df0[\"situacao_sql\"].astype(str).str.strip()\n",
    "        mask = st.isin(situacao)\n",
    "        df0 = df0[mask]\n",
    "        _count(\"STATUS\", df_before, df0, 'situacao_sql ∈ {\"Ativo Predial\",\"Ativo Territorial\"}', stats)\n",
    "\n",
    "    # -------- 3) valor_transacao_declarado\n",
    "    if \"valor_transacao_declarado\" in df0.columns:\n",
    "        df_before = df0\n",
    "        val = pd.to_numeric(df0[\"valor_transacao_declarado\"], errors=\"coerce\")\n",
    "        mask = (val >= min_valor) & (val <= max_valor)\n",
    "        df0 = df0[mask]\n",
    "        _count(\"VALOR\", df_before, df0, f\"{min_valor:,.0f} ≤ valor_transacao_declarado ≤ {max_valor:,.0f}\", stats)\n",
    "\n",
    "    # -------- 4) proporcao_transmitida_percent (qualidade, não trava em 100)\n",
    "    # Mantém apenas proporções plausíveis quando a coluna existir\n",
    "    if \"proporcao_transmitida_percent\" in df0.columns:\n",
    "        df_before = df0\n",
    "        prop = pd.to_numeric(df0[\"proporcao_transmitida_percent\"], errors=\"coerce\")\n",
    "        mask = (prop > 0) & (prop <= 100) | prop.isna()\n",
    "        df0 = df0[mask]\n",
    "        _count(\"PROP\", df_before, df0, \"0 < proporcao_transmitida_percent ≤ 100 (ou NA)\", stats)\n",
    "\n",
    "    # -------- 5) áreas coerentes por uso (descricao_uso_iptu)\n",
    "    # terreno -> area_terreno_m2>0 ; edificado -> area_construida_m2>0\n",
    "    # (opcional) permitir land_area em edificados (default True)\n",
    "    df_before = df0\n",
    "    built = pd.to_numeric(df0.get(\"area_construida_m2\", pd.Series(index=df0.index)), errors=\"coerce\").fillna(0)\n",
    "    land  = pd.to_numeric(df0.get(\"area_terreno_m2\",  pd.Series(index=df0.index)), errors=\"coerce\").fillna(0)\n",
    "    usage = df0.get(\"descricao_uso_iptu\", pd.Series(\"\", index=df0.index)).astype(str)\n",
    "\n",
    "    is_terreno = usage.str.contains(\"terren\", case=False, na=False)\n",
    "    is_edificado = ~is_terreno\n",
    "\n",
    "    mask_ok_terreno = (~is_terreno) | (land > 0)\n",
    "    mask_ok_edif    = (~is_edificado) | (built > 0)\n",
    "    mask = mask_ok_terreno & mask_ok_edif\n",
    "\n",
    "    if not allow_land_on_edificado:\n",
    "        mask = mask & (~(is_edificado & (land > 0)))\n",
    "        why_land = \" e (edificado → area_terreno_m2 = 0)\"\n",
    "    else:\n",
    "        why_land = \"\"\n",
    "\n",
    "    df0 = df0[mask]\n",
    "    _count(\"AREAS\", df_before, df0, f\"terreno → land>0; edificado → built>0{why_land}\", stats)\n",
    "\n",
    "    # -------- 6) ano_conclusao_construcao_iptu plausível (ou NA)\n",
    "    if \"ano_conclusao_construcao_iptu\" in df0.columns:\n",
    "        df_before = df0\n",
    "        ano = pd.to_numeric(df0[\"ano_conclusao_construcao_iptu\"], errors=\"coerce\")\n",
    "        ok = (ano >= ano_min) & (ano <= ano_max)\n",
    "        df0 = df0[ok | ano.isna()]\n",
    "        _count(\"ANO_CONS\", df_before, df0, f\"ano_conclusao_construcao_iptu ∈ [{ano_min}, {ano_max}] ou NA\", stats)\n",
    "\n",
    "    # -------- 7) data_transacao plausível (ou NA)\n",
    "    if \"data_transacao\" in df0.columns:\n",
    "        df_before = df0\n",
    "        td = pd.to_datetime(df0[\"data_transacao\"], errors=\"coerce\")\n",
    "        mask = td.between(\"2000-01-01\", pd.Timestamp.today()) | td.isna()\n",
    "        df0 = df0[mask]\n",
    "        _count(\"DATA\", df_before, df0, \"data_transacao ∈ [2000..hoje] ou NA\", stats)\n",
    "\n",
    "    # -------- 8) dedup por (cadastro_sql, data_transacao, valor_transacao_declarado)\n",
    "    df_before = df0\n",
    "    keys = [\"cadastro_sql\", \"data_transacao\", \"valor_transacao_declarado\"]\n",
    "    keys = [k for k in keys if k in df0.columns and df0[k].notna().any()]\n",
    "\n",
    "    if keys:\n",
    "        df0 = df0.sort_values(keys).drop_duplicates(subset=keys, keep=\"last\")\n",
    "        _count(\"DEDUP\", df_before, df0, f\"drop_duplicates por {keys}\", stats)\n",
    "    else:\n",
    "        _count(\"DEDUP\", df_before, df0, \"chaves ausentes; sem dedup\", stats)\n",
    "\n",
    "    audit = {\n",
    "        \"etapas\": stats,\n",
    "        \"linhas_iniciais\": len(df),\n",
    "        \"linhas_finais\": len(df0),\n",
    "        \"percentual_perdido\": 0.0 if len(df) == 0 else (len(df) - len(df0)) / len(df) * 100.0,\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[FIM] linhas iniciais={len(df):,} | finais={len(df0):,} | perda={audit['percentual_perdido']:.2f}%\")\n",
    "\n",
    "    return df0, audit\n",
    "\n",
    "\n",
    "df, audit = filtrar_base(\n",
    "    df,\n",
    "    include_permuta=False,\n",
    "    min_valor=100_000,\n",
    "    max_valor=100_000_000,\n",
    "    #situacao = [\"Ativo Territorial\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62009b67",
   "metadata": {},
   "source": [
    "### 5) Feature engineering (atributos temporais e do imóvel)\n",
    "\n",
    "Atributos criados:\n",
    "- `year`, `month`, `yyyymm`: granularidade temporal e chave mensal\n",
    "- `imovel_idade`: idade aproximada do imóvel (ano transação - ano conclusão)\n",
    "- `grupo_uso`: binarização simples de uso: TERRENO vs EDIFICADO\n",
    "\n",
    "**Nota:** `imovel_idade` é truncada em 0 para evitar negativos (dados inconsistentes).\n",
    "Valores ausentes são tratados como 0 (estratégia conservadora)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b0ed1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"data_transacao\"] = pd.to_datetime(df[\"data_transacao\"], errors=\"coerce\")\n",
    "df[\"year\"] = df[\"data_transacao\"].dt.year\n",
    "df[\"month\"] = df[\"data_transacao\"].dt.month\n",
    "df[\"yyyymm\"] = df[\"year\"] * 100 + df[\"month\"]\n",
    "\n",
    "# Idade do imóvel (ano de transação - ano de conclusão)\n",
    "df[\"imovel_idade\"] = df[\"year\"] - df[\"ano_conclusao_construcao_iptu\"]\n",
    "df[\"imovel_idade\"] = df[\"imovel_idade\"].clip(lower=0).fillna(0)\n",
    "\n",
    "# Grupo de uso (heurística por descrição)\n",
    "df[\"grupo_uso\"] = np.where(\n",
    "    df[\"descricao_uso_iptu\"].str.contains(\"terren\", case=False, na=False),\n",
    "    \"TERRENO\",\n",
    "    \"EDIFICADO\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf10fea",
   "metadata": {},
   "source": [
    "### 6) Agregados mensais por setor fiscal (com defasagem)\n",
    "\n",
    "Nesta etapa criamos variáveis de contexto de mercado, por **mês**, **setor fiscal** e **grupo de uso**.\n",
    "\n",
    "Métricas mensais:\n",
    "- `median_total`: mediana do valor declarado no grupo\n",
    "- `median_sqm`: (atualmente igual à mediana do total; ver observação abaixo)\n",
    "- `count`: quantidade de transações no mês (proxy de liquidez)\n",
    "\n",
    "Em seguida, calculamos `lag-1` (mês anterior) para evitar vazamento temporal:\n",
    "- `median_total_lag1`, `median_sqm_lag1`, `count_lag1`\n",
    "\n",
    "**Observação importante:** `median_sqm` está calculado com a mesma expressão de `median_total`.\n",
    "Se a intenção for mediana do preço por m², o correto seria:\n",
    "`declared_transaction_value / built_area_sqm` (ou outra área relevante), com tratamento de divisão por zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62623d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "monthly = (\n",
    "    df.groupby([\"grupo_uso\", \"setor_fiscal\", \"yyyymm\"])\n",
    "      .agg(\n",
    "          median_total=(\"valor_transacao_declarado\", \"median\"),\n",
    "          median_sqm=(\"valor_transacao_declarado\", lambda x: np.median(x)),\n",
    "          count=(\"valor_transacao_declarado\", \"count\")\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "monthly[\"median_total_lag1\"] = (\n",
    "    monthly.sort_values(\"yyyymm\")\n",
    "           .groupby([\"grupo_uso\", \"setor_fiscal\"])[\"median_total\"]\n",
    "           .shift(1)\n",
    ")\n",
    "\n",
    "monthly[\"median_sqm_lag1\"] = (\n",
    "    monthly.sort_values(\"yyyymm\")\n",
    "           .groupby([\"grupo_uso\", \"setor_fiscal\"])[\"median_sqm\"]\n",
    "           .shift(1)\n",
    ")\n",
    "\n",
    "monthly[\"count_lag1\"] = (\n",
    "    monthly.sort_values(\"yyyymm\")\n",
    "           .groupby([\"grupo_uso\", \"setor_fiscal\"])[\"count\"]\n",
    "           .shift(1)\n",
    ")\n",
    "\n",
    "df = df.merge(\n",
    "    monthly[[\n",
    "        \"grupo_uso\",\n",
    "        \"setor_fiscal\",\n",
    "        \"yyyymm\",\n",
    "        \"median_total_lag1\",\n",
    "        \"median_sqm_lag1\",\n",
    "        \"count_lag1\"\n",
    "    ]],\n",
    "    on=[\"grupo_uso\", \"setor_fiscal\", \"yyyymm\"],\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff6c971",
   "metadata": {},
   "source": [
    "### 7) Definição do target (log-transform)\n",
    "\n",
    "Para reduzir assimetria (cauda longa) típica de valores imobiliários:\n",
    "- Definimos `target = log1p(valor / TARGET_SCALE)`\n",
    "\n",
    "No momento da avaliação (predição), desfazemos a transformação:\n",
    "- `valor_pred = expm1(pred) * TARGET_SCALE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d675a4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"target\"] = np.log1p(df[\"valor_transacao_declarado\"] / TARGET_SCALE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd82225e",
   "metadata": {},
   "source": [
    "### 8) Criação das faixas de valor\n",
    "\n",
    "Segmenta cada registro em uma faixa baseada no `valor_transacao_declarado`.\n",
    "Isso permite treinar modelos especializados por regime de preços."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5162e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"faixa\"] = pd.cut(\n",
    "    df[\"valor_transacao_declarado\"],\n",
    "    bins=FAIXAS_BINS,\n",
    "    labels=FAIXAS_LABELS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10b414e",
   "metadata": {},
   "source": [
    "### 9) Treinamento dos modelos (KFold 5x)\n",
    "\n",
    "Para cada `grupo_uso` e `faixa`:\n",
    "- Filtra o subconjunto (`sub`)\n",
    "- Se houver menos de 1000 registros, pula (amostra insuficiente)\n",
    "- Cria matriz `X` com:\n",
    "  - Categóricas: `CAT_COLS`\n",
    "  - Numéricas: área construída, área do terreno, testada, idade, yyyymm, lags do setor\n",
    "- Realiza validação cruzada KFold (5 folds)\n",
    "- Métricas por fold:\n",
    "  - **MAE** em R$ (no espaço original, após inversão do log)\n",
    "  - **R²** (coeficiente de determinação)\n",
    "\n",
    "Ao final, salva o último modelo treinado da iteração em:\n",
    "`artefatos_modelo_multifaixas/modelo_<grupo>_<faixa>.cbm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11aa0a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treinando EDIFICADO - faixa1_<=500k (989225 linhas)\n",
      "[Fold 1] MAE: R$ 61,422 | R²: 0.450\n",
      "[Fold 2] MAE: R$ 61,619 | R²: 0.447\n",
      "[Fold 3] MAE: R$ 61,602 | R²: 0.446\n",
      "[Fold 4] MAE: R$ 61,507 | R²: 0.447\n",
      "[Fold 5] MAE: R$ 61,444 | R²: 0.448\n",
      ">> MÉDIA MAE: R$ 61,519 | R²: 0.448\n",
      "\n",
      "Treinando EDIFICADO - faixa2_500k_2mi (338490 linhas)\n",
      "[Fold 1] MAE: R$ 195,554 | R²: 0.450\n",
      "[Fold 2] MAE: R$ 194,489 | R²: 0.453\n",
      "[Fold 3] MAE: R$ 196,150 | R²: 0.444\n",
      "[Fold 4] MAE: R$ 194,611 | R²: 0.447\n",
      "[Fold 5] MAE: R$ 196,797 | R²: 0.441\n",
      ">> MÉDIA MAE: R$ 195,520 | R²: 0.447\n",
      "\n",
      "Treinando EDIFICADO - faixa3_2mi_10mi (104190 linhas)\n",
      "[Fold 1] MAE: R$ 1,272,800 | R²: 0.371\n",
      "[Fold 2] MAE: R$ 1,268,748 | R²: 0.369\n",
      "[Fold 3] MAE: R$ 1,277,423 | R²: 0.369\n",
      "[Fold 4] MAE: R$ 1,263,007 | R²: 0.370\n",
      "[Fold 5] MAE: R$ 1,263,986 | R²: 0.381\n",
      ">> MÉDIA MAE: R$ 1,269,193 | R²: 0.372\n",
      "\n",
      "Treinando EDIFICADO - faixa4_10mi_30mi (78045 linhas)\n",
      "[Fold 1] MAE: R$ 3,070,967 | R²: 0.502\n",
      "[Fold 2] MAE: R$ 3,062,681 | R²: 0.510\n",
      "[Fold 3] MAE: R$ 3,040,863 | R²: 0.515\n",
      "[Fold 4] MAE: R$ 3,023,715 | R²: 0.512\n",
      "[Fold 5] MAE: R$ 3,025,681 | R²: 0.510\n",
      ">> MÉDIA MAE: R$ 3,044,781 | R²: 0.510\n",
      "\n",
      "Treinando EDIFICADO - faixa5_30mi_100mi (66774 linhas)\n",
      "[Fold 1] MAE: R$ 8,730,931 | R²: 0.494\n",
      "[Fold 2] MAE: R$ 8,659,411 | R²: 0.502\n",
      "[Fold 3] MAE: R$ 8,649,556 | R²: 0.499\n",
      "[Fold 4] MAE: R$ 8,946,949 | R²: 0.489\n",
      "[Fold 5] MAE: R$ 8,806,195 | R²: 0.491\n",
      ">> MÉDIA MAE: R$ 8,758,608 | R²: 0.495\n",
      "\n",
      "Treinando TERRENO - faixa1_<=500k (52680 linhas)\n",
      "[Fold 1] MAE: R$ 44,790 | R²: 0.393\n",
      "[Fold 2] MAE: R$ 43,431 | R²: 0.415\n",
      "[Fold 3] MAE: R$ 43,818 | R²: 0.427\n",
      "[Fold 4] MAE: R$ 45,237 | R²: 0.390\n",
      "[Fold 5] MAE: R$ 44,632 | R²: 0.412\n",
      ">> MÉDIA MAE: R$ 44,382 | R²: 0.408\n",
      "\n",
      "Treinando TERRENO - faixa2_500k_2mi (10703 linhas)\n",
      "[Fold 1] MAE: R$ 244,861 | R²: 0.362\n",
      "[Fold 2] MAE: R$ 255,195 | R²: 0.348\n",
      "[Fold 3] MAE: R$ 238,344 | R²: 0.367\n",
      "[Fold 4] MAE: R$ 251,533 | R²: 0.329\n",
      "[Fold 5] MAE: R$ 253,070 | R²: 0.336\n",
      ">> MÉDIA MAE: R$ 248,601 | R²: 0.348\n",
      "\n",
      "Treinando TERRENO - faixa3_2mi_10mi (8758 linhas)\n",
      "[Fold 1] MAE: R$ 1,056,591 | R²: 0.289\n",
      "[Fold 2] MAE: R$ 1,018,270 | R²: 0.324\n",
      "[Fold 3] MAE: R$ 1,046,356 | R²: 0.281\n",
      "[Fold 4] MAE: R$ 1,125,328 | R²: 0.276\n",
      "[Fold 5] MAE: R$ 1,080,230 | R²: 0.280\n",
      ">> MÉDIA MAE: R$ 1,065,355 | R²: 0.290\n",
      "\n",
      "Treinando TERRENO - faixa4_10mi_30mi (18533 linhas)\n",
      "[Fold 1] MAE: R$ 1,780,880 | R²: 0.466\n",
      "[Fold 2] MAE: R$ 1,852,229 | R²: 0.441\n",
      "[Fold 3] MAE: R$ 1,904,164 | R²: 0.433\n",
      "[Fold 4] MAE: R$ 1,826,175 | R²: 0.446\n",
      "[Fold 5] MAE: R$ 1,864,542 | R²: 0.425\n",
      ">> MÉDIA MAE: R$ 1,845,598 | R²: 0.442\n",
      "\n",
      "Treinando TERRENO - faixa5_30mi_100mi (17546 linhas)\n",
      "[Fold 1] MAE: R$ 9,048,403 | R²: 0.502\n",
      "[Fold 2] MAE: R$ 9,051,651 | R²: 0.502\n",
      "[Fold 3] MAE: R$ 9,142,798 | R²: 0.492\n",
      "[Fold 4] MAE: R$ 9,003,824 | R²: 0.516\n",
      "[Fold 5] MAE: R$ 9,229,070 | R²: 0.493\n",
      ">> MÉDIA MAE: R$ 9,095,149 | R²: 0.501\n"
     ]
    }
   ],
   "source": [
    "CAT_NA_TOKEN = \"__MISSING__\"\n",
    "for c in CAT_COLS:\n",
    "    df[c] = df[c].astype(\"string\").fillna(CAT_NA_TOKEN).astype(str)\n",
    "\n",
    "\n",
    "modelos = {}\n",
    "\n",
    "for grupo in df[\"grupo_uso\"].unique():\n",
    "    modelos[grupo] = {}\n",
    "\n",
    "    for faixa in FAIXAS_LABELS:\n",
    "        sub = df[(df[\"grupo_uso\"] == grupo) & (df[\"faixa\"] == faixa)]\n",
    "\n",
    "        # Evita treinar com pouca amostra\n",
    "        if len(sub) < 1000:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nTreinando {grupo} - {faixa} ({len(sub)} linhas)\")\n",
    "\n",
    "        X = sub[CAT_COLS + [\n",
    "            \"area_construida_m2\",\n",
    "            \"area_terreno_m2\",\n",
    "            \"testada_m\",\n",
    "            \"imovel_idade\",\n",
    "            \"yyyymm\",\n",
    "            \"median_total_lag1\",\n",
    "            \"median_sqm_lag1\",\n",
    "            \"count_lag1\"\n",
    "        ]]\n",
    "\n",
    "        y = sub[\"target\"]\n",
    "\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        maes = []\n",
    "        r2s = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "            model = CatBoostRegressor(\n",
    "                iterations=400,\n",
    "                depth=6,\n",
    "                learning_rate=0.05,\n",
    "                loss_function=\"RMSE\",\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            model.fit(\n",
    "                Pool(X_train, y_train, cat_features=CAT_COLS),\n",
    "                eval_set=Pool(X_val, y_val, cat_features=CAT_COLS),\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            # volta do log para R$\n",
    "            pred = np.expm1(model.predict(X_val)) * TARGET_SCALE\n",
    "            true = np.expm1(y_val) * TARGET_SCALE\n",
    "\n",
    "            mae = mean_absolute_error(true, pred)\n",
    "            r2 = r2_score(true, pred)\n",
    "\n",
    "            maes.append(mae)\n",
    "            r2s.append(r2)\n",
    "\n",
    "            print(f\"[Fold {fold}] MAE: R$ {mae:,.0f} | R²: {r2:.3f}\")\n",
    "\n",
    "        print(f\">> MÉDIA MAE: R$ {np.mean(maes):,.0f} | R²: {np.mean(r2s):.3f}\")\n",
    "\n",
    "        # guarda e salva o último modelo treinado\n",
    "        modelos[grupo][faixa] = model\n",
    "\n",
    "        model.save_model(\n",
    "            f\"{ARTEFATOS_PATH}/modelo_{grupo}_{faixa}.cbm\"\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
